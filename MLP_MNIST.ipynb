{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FhHorHBo7hG9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "batch_size = 64\n",
        "num_epochs = 500\n",
        "hidden_layers = [128, 64]\n",
        "act_fn = 's' #sigmoid or relu\n",
        "lr = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qN4ZD6Xvbe9"
      },
      "source": [
        "#Loading MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FKVwajeIvlsx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGAu-qGy0Ivr",
        "outputId": "4288f654-7f27-4671-e3fb-e6834c8a4729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(54000, 28, 28)\n",
            "(6000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "val_start = int(train_X.shape[0] * 0.9)\n",
        "val_X = train_X[val_start:]\n",
        "val_y = train_y[val_start:]\n",
        "train_X = train_X[:val_start]\n",
        "train_y = train_y[:val_start]\n",
        "print(train_X.shape)\n",
        "print(val_X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WgKs26iEv53e"
      },
      "outputs": [],
      "source": [
        "train_X = train_X.reshape((train_X.shape[0], -1))\n",
        "test_X = test_X.reshape((test_X.shape[0], -1))\n",
        "val_X = val_X.reshape((val_X.shape[0], -1))\n",
        "train_X = train_X.astype('float32') / 255\n",
        "test_X = test_X.astype('float32') / 255\n",
        "val_X = val_X.astype('float32') / 255\n",
        "\n",
        "train_y = to_categorical(train_y, num_classes=10)\n",
        "val_y = to_categorical(val_y, num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Gshjw-Ndv3M4"
      },
      "outputs": [],
      "source": [
        "def batch_generator(X, y, batch_size=batch_size):\n",
        "    \"\"\"Generate batches of data.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    # Loop over the data in increments of batch_size\n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        yield X[start:end], y[start:end]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qu4awrg1Mu5"
      },
      "source": [
        "#MLP Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VWwps9wlJjn-"
      },
      "outputs": [],
      "source": [
        "#Math Functions\n",
        "\n",
        "def relu(z):\n",
        "  return np.maximum(0, z)\n",
        "\n",
        "def relu_deriv(z):\n",
        "  return (z>0).astype(float)\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_deriv(z):\n",
        "  s = sigmoid(z)\n",
        "  return s * (1 - s)\n",
        "\n",
        "def softmax(z):\n",
        "  z_max = np.max(z, axis=1, keepdims=True)\n",
        "  e_z = np.exp(z - z_max)\n",
        "  return e_z / np.sum(e_z, axis=1, keepdims=True)\n",
        "\n",
        "def celoss(pred, tru):\n",
        "  loss = -np.sum(tru * np.log(pred))\n",
        "  return loss\n",
        "\n",
        "def celoss_deriv(a, y):\n",
        "  return a - y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8Lq1ma9_sG-x"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "  def __init__(self):\n",
        "    self.w = {}\n",
        "    self.b = {}\n",
        "    self.layer_sizes = [28*28] + hidden_layers + [10]\n",
        "\n",
        "    #initialize weights\n",
        "    for i in range(len(self.layer_sizes) - 1):\n",
        "\n",
        "      n_in = self.layer_sizes[i]\n",
        "      n_out = self.layer_sizes[i + 1]\n",
        "\n",
        "      if act_fn == 's':\n",
        "        #Xavier initialization\n",
        "        self.act = sigmoid\n",
        "        self.act_deriv = sigmoid_deriv\n",
        "        weight_matrix = np.random.randn(n_in, n_out)  * np.sqrt(2 / (n_in + n_out))\n",
        "      else:\n",
        "        #He initialization\n",
        "        self.act = relu\n",
        "        self.act_deriv = relu_deriv\n",
        "        weight_matrix = np.random.randn(n_in, n_out)  * np.sqrt(2 / n_in)\n",
        "\n",
        "      self.w[i] = (weight_matrix)\n",
        "      self.b[i] = (np.zeros((1, n_out)))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    act = self.act\n",
        "    z = {}\n",
        "    a = {}\n",
        "    a[0] = x\n",
        "\n",
        "    for i in range(len(self.layer_sizes) - 1):\n",
        "      idx = i+1\n",
        "      if idx == len(self.layer_sizes) - 1:\n",
        "        act = softmax\n",
        "      z[idx] = a[i] @ self.w[i] + self.b[i]\n",
        "      a[idx] = act(z[idx])\n",
        "    return z, a\n",
        "\n",
        "  def backprop(self, z, a, y):\n",
        "    dz3 = celoss_deriv(a[3], y) #(B, 10)\n",
        "    db2 = np.mean(dz3, axis = 0) #(B, 10)\n",
        "    dw2 = a[2].T @ dz3 #(64,10)\n",
        "    dw2/= y.shape[0] #(64,10)\n",
        "\n",
        "    self.b[2] -= lr * db2\n",
        "    self.w[2] -= lr * dw2\n",
        "\n",
        "    da2 = dz3 @ self.w[2].T #(B, 64)\n",
        "    dz2 = np.multiply(self.act_deriv(z[2]), da2) #(B, 64)\n",
        "\n",
        "    db1 = np.mean(dz2, axis=0) #(B, 64)\n",
        "    dw1 = a[1].T @ dz2 #(128, 64)\n",
        "    dw1/= y.shape[0]\n",
        "\n",
        "    self.b[1] -= lr * db1\n",
        "    self.w[1] -= lr * dw1\n",
        "\n",
        "    da1 = dz2 @ self.w[1].T #(B, 128)\n",
        "    dz1 = np.multiply(self.act_deriv(z[1]), da1) #(B, 128)\n",
        "\n",
        "    db0 = np.mean(dz1, axis=0) #(B, 128)\n",
        "    dw0 = a[0].T @ dz1 #(784, 128)\n",
        "    dw0 /= y.shape[0]\n",
        "\n",
        "    self.b[0] -= lr * db0\n",
        "    self.w[0] -= lr * dw0\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFmPo88S-B-x",
        "outputId": "bc4f17e8-6d9c-4e4a-dec6-85925f1a765b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/500 done. Train loss: 0.6759495455081844, Val loss: 0.577732324012872\n",
            "Epoch 20/500 done. Train loss: 0.4023590900289391, Val loss: 0.3335779931852674\n",
            "Epoch 30/500 done. Train loss: 0.3368603682415012, Val loss: 0.2779079500287303\n",
            "Epoch 40/500 done. Train loss: 0.3023537242929158, Val loss: 0.24940889368148328\n",
            "Epoch 50/500 done. Train loss: 0.2776805312650549, Val loss: 0.22881294471760288\n",
            "Epoch 60/500 done. Train loss: 0.2575754850915566, Val loss: 0.21197445994272723\n",
            "Epoch 70/500 done. Train loss: 0.24001553275105023, Val loss: 0.19786381134378087\n",
            "Epoch 80/500 done. Train loss: 0.22399133010078218, Val loss: 0.18479183971776839\n",
            "Epoch 90/500 done. Train loss: 0.20964426925082255, Val loss: 0.17389427256491619\n",
            "Epoch 100/500 done. Train loss: 0.19654072057157781, Val loss: 0.16350662913021316\n",
            "Epoch 110/500 done. Train loss: 0.18447714711238805, Val loss: 0.15485020124615118\n",
            "Epoch 120/500 done. Train loss: 0.1735942680689455, Val loss: 0.14647707054447015\n",
            "Epoch 130/500 done. Train loss: 0.16358368258867265, Val loss: 0.1385894872818425\n",
            "Epoch 140/500 done. Train loss: 0.15444779603914888, Val loss: 0.13235121700189206\n",
            "Epoch 150/500 done. Train loss: 0.14612238012924297, Val loss: 0.12623828273452542\n",
            "Epoch 160/500 done. Train loss: 0.13840822863245011, Val loss: 0.12120195182475664\n",
            "Epoch 170/500 done. Train loss: 0.13131755233942102, Val loss: 0.11677381073076262\n",
            "No improvement in validation loss for 1 epochs. At Epoch 175/500.\n",
            "Epoch 180/500 done. Train loss: 0.12469542483222873, Val loss: 0.11230789384223012\n",
            "No improvement in validation loss for 1 epochs. At Epoch 181/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 183/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 184/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 189/500.\n",
            "Epoch 190/500 done. Train loss: 0.11874335609036751, Val loss: 0.10912000420824211\n",
            "Epoch 200/500 done. Train loss: 0.11301867230368272, Val loss: 0.10582280807619682\n",
            "No improvement in validation loss for 1 epochs. At Epoch 200/500.\n",
            "Epoch 210/500 done. Train loss: 0.10779972664247503, Val loss: 0.10215648903025441\n",
            "No improvement in validation loss for 1 epochs. At Epoch 213/500.\n",
            "Epoch 220/500 done. Train loss: 0.1029741865923917, Val loss: 0.0996268814871752\n",
            "No improvement in validation loss for 1 epochs. At Epoch 225/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 226/500.\n",
            "Epoch 230/500 done. Train loss: 0.09842263220490415, Val loss: 0.09727849772598593\n",
            "No improvement in validation loss for 1 epochs. At Epoch 234/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 237/500.\n",
            "Epoch 240/500 done. Train loss: 0.09422758268315314, Val loss: 0.09492848852885216\n",
            "No improvement in validation loss for 1 epochs. At Epoch 245/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 247/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 249/500.\n",
            "Epoch 250/500 done. Train loss: 0.0902248225771656, Val loss: 0.0924687478057136\n",
            "No improvement in validation loss for 1 epochs. At Epoch 251/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 252/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 257/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 258/500.\n",
            "Epoch 260/500 done. Train loss: 0.0865507221424413, Val loss: 0.09083689188298814\n",
            "No improvement in validation loss for 1 epochs. At Epoch 262/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 265/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 267/500.\n",
            "Epoch 270/500 done. Train loss: 0.0830375542473068, Val loss: 0.08931421031972038\n",
            "No improvement in validation loss for 1 epochs. At Epoch 270/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 271/500.\n",
            "No improvement in validation loss for 3 epochs. At Epoch 272/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 276/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 277/500.\n",
            "No improvement in validation loss for 3 epochs. At Epoch 278/500.\n",
            "Epoch 280/500 done. Train loss: 0.07972624307891578, Val loss: 0.08750889002016078\n",
            "No improvement in validation loss for 1 epochs. At Epoch 287/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 288/500.\n",
            "Epoch 290/500 done. Train loss: 0.0766423215988142, Val loss: 0.08593463648013534\n",
            "No improvement in validation loss for 1 epochs. At Epoch 291/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 293/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 295/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 297/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 299/500.\n",
            "Epoch 300/500 done. Train loss: 0.07368298878176825, Val loss: 0.08436891914918927\n",
            "No improvement in validation loss for 2 epochs. At Epoch 300/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 302/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 308/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 309/500.\n",
            "Epoch 310/500 done. Train loss: 0.07095219933785157, Val loss: 0.08306821220298292\n",
            "No improvement in validation loss for 1 epochs. At Epoch 314/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 315/500.\n",
            "No improvement in validation loss for 3 epochs. At Epoch 316/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 319/500.\n",
            "Epoch 320/500 done. Train loss: 0.06822575078201795, Val loss: 0.08168038680968906\n",
            "No improvement in validation loss for 1 epochs. At Epoch 322/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 323/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 325/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 326/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 328/500.\n",
            "Epoch 330/500 done. Train loss: 0.06577315550309737, Val loss: 0.08073983001343325\n",
            "No improvement in validation loss for 1 epochs. At Epoch 331/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 332/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 334/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 336/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 337/500.\n",
            "No improvement in validation loss for 3 epochs. At Epoch 338/500.\n",
            "No improvement in validation loss for 4 epochs. At Epoch 339/500.\n",
            "Epoch 340/500 done. Train loss: 0.06337896001059244, Val loss: 0.0795268850395505\n",
            "No improvement in validation loss for 1 epochs. At Epoch 341/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 342/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 347/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 348/500.\n",
            "Epoch 350/500 done. Train loss: 0.061106943951310104, Val loss: 0.07849943307788604\n",
            "No improvement in validation loss for 1 epochs. At Epoch 351/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 354/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 355/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 358/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 359/500.\n",
            "Epoch 360/500 done. Train loss: 0.05892159762796274, Val loss: 0.07757206544266845\n",
            "No improvement in validation loss for 1 epochs. At Epoch 361/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 363/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 364/500.\n",
            "No improvement in validation loss for 3 epochs. At Epoch 365/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 367/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 368/500.\n",
            "Epoch 370/500 done. Train loss: 0.05682012465942643, Val loss: 0.07628936406994434\n",
            "No improvement in validation loss for 1 epochs. At Epoch 371/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 372/500.\n",
            "No improvement in validation loss for 3 epochs. At Epoch 373/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 376/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 377/500.\n",
            "No improvement in validation loss for 3 epochs. At Epoch 378/500.\n",
            "Epoch 380/500 done. Train loss: 0.054883076321915115, Val loss: 0.0759838228259893\n",
            "No improvement in validation loss for 1 epochs. At Epoch 381/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 384/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 385/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 387/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 388/500.\n",
            "No improvement in validation loss for 3 epochs. At Epoch 389/500.\n",
            "Epoch 390/500 done. Train loss: 0.052942031249214115, Val loss: 0.07530623835940202\n",
            "No improvement in validation loss for 4 epochs. At Epoch 390/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 392/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 393/500.\n",
            "No improvement in validation loss for 3 epochs. At Epoch 394/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 397/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 399/500.\n",
            "Epoch 400/500 done. Train loss: 0.051117039522561504, Val loss: 0.07451557751522747\n",
            "No improvement in validation loss for 1 epochs. At Epoch 403/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 405/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 407/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 408/500.\n",
            "No improvement in validation loss for 3 epochs. At Epoch 409/500.\n",
            "Epoch 410/500 done. Train loss: 0.049412090484742655, Val loss: 0.07371597639701098\n",
            "No improvement in validation loss for 1 epochs. At Epoch 412/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 413/500.\n",
            "No improvement in validation loss for 3 epochs. At Epoch 414/500.\n",
            "No improvement in validation loss for 4 epochs. At Epoch 415/500.\n",
            "No improvement in validation loss for 1 epochs. At Epoch 417/500.\n",
            "No improvement in validation loss for 2 epochs. At Epoch 418/500.\n",
            "No improvement in validation loss for 3 epochs. At Epoch 419/500.\n",
            "Epoch 420/500 done. Train loss: 0.047705847360790844, Val loss: 0.07333838670086432\n",
            "No improvement in validation loss for 4 epochs. At Epoch 420/500.\n",
            "No improvement in validation loss for 5 epochs. At Epoch 421/500.\n",
            "Early Stopping Triggered!\n"
          ]
        }
      ],
      "source": [
        "#training\n",
        "model = MLP()\n",
        "\n",
        "best_val_loss = np.inf\n",
        "patience = 5\n",
        "\n",
        "# Variables to hold the best model state\n",
        "best_w = None\n",
        "best_b = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  indices = np.arange(train_X.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  train_X = train_X[indices]\n",
        "  train_y = train_y[indices]\n",
        "\n",
        "  running_loss = 0\n",
        "  for X_batch, y_batch in batch_generator(train_X, train_y, batch_size):\n",
        "\n",
        "    z,a = model.forward(X_batch)\n",
        "    running_loss += celoss(a[3], y_batch)\n",
        "    model.backprop(z, a, y_batch)\n",
        "\n",
        "  val_loss = 0\n",
        "  for X_batch, y_batch in batch_generator(val_X, val_y, batch_size):\n",
        "    z,a = model.forward(X_batch)\n",
        "    val_loss += celoss(a[3], y_batch)\n",
        "\n",
        "  if (epoch+1)%10 == 0 or epoch == 0:\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} done. Train loss: {running_loss/train_X.shape[0]}, Val loss: {val_loss/val_X.shape[0]}\")\n",
        "\n",
        "  if val_loss/val_X.shape[0] < best_val_loss:\n",
        "    best_val_loss = val_loss/val_X.shape[0]\n",
        "    best_w = deepcopy(model.w)\n",
        "    best_b = deepcopy(model.b)\n",
        "    epochs_no_improve = 0\n",
        "  else:\n",
        "    epochs_no_improve += 1\n",
        "    print(f\"No improvement in validation loss for {epochs_no_improve} epochs. At Epoch {epoch+1}/{num_epochs}.\")\n",
        "\n",
        "  if epochs_no_improve == patience:\n",
        "    print(\"Early Stopping Triggered!\")\n",
        "    break\n",
        "\n",
        "model.w = best_w\n",
        "model.b = best_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PFxeSEVdyUn",
        "outputId": "658ed3ac-d525-42bd-c376-771fa770d65f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 97.53%\n"
          ]
        }
      ],
      "source": [
        "#testing\n",
        "num_correct = 0\n",
        "num_samples = 0\n",
        "for X_batch, y_batch in batch_generator(test_X, test_y, batch_size):\n",
        "  z,a = model.forward(X_batch)\n",
        "  predictions = np.argmax(a[3], axis = 1)\n",
        "  num_correct += np.sum(predictions == y_batch)\n",
        "  num_samples += y_batch.shape[0]\n",
        "\n",
        "accuracy = num_correct/ num_samples\n",
        "print(f\"Model Accuracy: {accuracy*100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1lU9Asui2Rb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "d8FGDrfHvfCG",
        "EdR7mxmlE_UV"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
